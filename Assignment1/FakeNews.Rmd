---
title: "Trump Fakes and Fox: Political Content Detection"
author: "Randy Howk"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)

# Load libraries optimized for large datasets
library(dplyr)
library(readr)
library(stringr)
library(lubridate)
library(ggplot2)
library(knitr)
library(data.table)  # Faster for large datasets
library(dtplyr)      # dplyr backend for data.table
library(scales)      # For better plot formatting

# Set options for large dataset handling
options(scipen = 999)  # Avoid scientific notation
```

### **Overview Section**

```r
## Overview

This analysis examines a large dataset of political news articles to develop automated classification methods for identifying Trump-related and Fox News-related content. The dataset spans multiple years of political coverage during a period of significant media activity and political events. This work builds upon research in automated news categorization and applies scalable text processing techniques to handle large-scale news corpora. The methodology demonstrates efficient processing of substantial datasets while maintaining classification accuracy for media bias detection and political sentiment tracking.
```

```{r load-data}
# Efficient data loading for large files
# Replace "your_large_dataset.csv" with your actual filename
cat("Loading large dataset...\n")

# Option 1: Using data.table for speed (recommended for very large files)
# data <- fread("your_large_dataset.csv")

# Option 2: Using readr with progress bar and column types
data <- read_csv("Fake.csv", 
                 col_types = cols(
                   title = col_character(),
                   text = col_character(),
                   subject = col_character(),
                   date = col_character()
                 ),
                 progress = TRUE)

# Convert to lazy data.table for faster operations
data <- lazy_dt(data)

# Display basic dataset information
cat("Dataset dimensions:", nrow(data), "rows,", ncol(data), "columns\n")
cat("Memory usage:", format(object.size(data), units = "MB"), "\n")

# Sample preview (don't show all rows for large datasets)
sample_data <- data %>% 
  slice_sample(n = 5) %>% 
  select(title, subject, date) %>% 
  collect()

kable(sample_data, caption = "Random Sample from Dataset")

# Check date range
date_range <- data %>% 
  summarise(
    earliest = min(date, na.rm = TRUE),
    latest = max(date, na.rm = TRUE),
    .groups = 'drop'
  ) %>% 
  collect()

cat("Date range:", date_range$earliest, "to", date_range$latest, "\n")
```


### **Data Cleaning (Optimized for Performance)**


```{r data-cleaning}
cat("Processing large dataset - this may take a few minutes...\n")

library(lubridate)

# Efficient text processing for large datasets
cleaned_data <- data %>%
  # Filter out any problematic rows early
  filter(!is.na(title), !is.na(text), nchar(text) > 100) %>%
  
  # Create computed columns efficiently
  mutate(
    # Convert date once
    publication_date = dmy(date),
    publication_month = month(publication_date),
    publication_year = year(publication_date),
    
    # Use vectorized string operations
    title_lower = str_to_lower(title),
    text_lower = str_to_lower(text),
    
    # Trump classification (case-insensitive)
    trump_related = case_when(
      str_detect(title_lower, "trump|donald") | 
      str_detect(text_lower, "trump|donald") ~ "Trump-Related",
      TRUE ~ "Other-Politics"
    ),
    
    # Fox News classification (expanded terms)
    fox_related = case_when(
      str_detect(title_lower, "fox|fox news") | 
      str_detect(text_lower, "fox news|fox & friends|hannity|tucker carlson|sean hannity|laura ingraham|fox business") ~ "Fox-Related",
      TRUE ~ "Non-Fox"
    ),
    
    # Combined classification
    political_focus = case_when(
      trump_related == "Trump-Related" & fox_related == "Fox-Related" ~ "Trump-Fox",
      trump_related == "Trump-Related" & fox_related == "Non-Fox" ~ "Trump-Only", 
      trump_related == "Other-Politics" & fox_related == "Fox-Related" ~ "Fox-Only",
      TRUE ~ "General-Politics"
    ),
    
    # Efficient text metrics
    word_count = str_count(text, "\\S+"),
    title_word_count = str_count(title, "\\S+"),
    
    # Article length categories
    article_length = case_when(
      word_count < 500 ~ "Short",
      word_count < 1500 ~ "Medium",
      word_count < 3000 ~ "Long",
      TRUE ~ "Very-Long"
    ),
    
    # Temporal features
    publication_year = year(publication_date),
    publication_month = month(publication_date),
    publication_day = wday(publication_date),
    
    # Political keyword density (efficiency matters for large datasets)
    political_keywords = str_count(text_lower, 
      "republican|democrat|senate|congress|president|election|vote|politics|campaign|gop")
  ) %>%
  
  # Remove temporary columns to save memory
  select(-title_lower, -text_lower) %>%
  
  # Collect results (convert from lazy evaluation)
  collect()

# Progress update
cat("Data cleaning complete!\n")
cat("Classification summary:\n")
summary_stats <- cleaned_data %>%
  count(trump_related, fox_related, name = "articles") %>%
  arrange(desc(articles))

print(summary_stats)
```

### **Create Final Dataset (Memory Efficient)**


```{r final-dataset}
# Create optimized final dataset
final_data <- cleaned_data %>%
  select(
    title,
    trump_related,
    fox_related, 
    political_focus,
    publication_date,
    publication_year,
    publication_month,
    publication_day,
    article_length,
    word_count,
    title_word_count,
    political_keywords,
    text  # Keep for reference but consider removing if memory is tight
  ) %>%
  arrange(publication_date)

# Comprehensive summary for large dataset
summary_stats <- final_data %>%
  summarise(
    total_articles = n(),
    trump_articles = sum(trump_related == "Trump-Related"),
    fox_articles = sum(fox_related == "Fox-Related"),
    avg_word_count = round(mean(word_count, na.rm = TRUE), 0),
    median_word_count = median(word_count, na.rm = TRUE),
    date_span_days = as.numeric(max(publication_date) - min(publication_date)),
    years_covered = n_distinct(publication_year),
    .groups = 'drop'
  )

kable(summary_stats, caption = "Large Dataset Summary Statistics")

# Show classification breakdown
classification_breakdown <- final_data %>%
  count(political_focus, sort = TRUE) %>%
  mutate(percentage = round(100 * n / sum(n), 1))

kable(classification_breakdown, 
      caption = "Article Classification Distribution",
      col.names = c("Political Focus", "Count", "Percentage"))
```

### **Efficient Visualizations for Large Data**


```{r eda-graphics, fig.width=10, fig.height=6}
# Sample data for plotting if dataset is very large (>10k rows)
plot_data <- if(nrow(final_data) > 10000) {
  final_data %>% slice_sample(n = 10000)
} else {
  final_data
}

# 1. Classification distribution
p1 <- ggplot(final_data, aes(x = political_focus, fill = article_length)) +
  geom_bar(position = "stack") +
  labs(title = "Article Distribution by Political Focus and Length",
       subtitle = paste("Total articles:", nrow(final_data)),
       x = "Political Focus Category",
       y = "Number of Articles",
       fill = "Article Length") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(labels = comma)

print(p1)

# 2. Temporal trends (aggregated by month for performance)
temporal_data <- final_data %>%
  mutate(year_month = floor_date(publication_date, "month")) %>%
  count(year_month, political_focus) %>%
  filter(!is.na(year_month))

p2 <- ggplot(temporal_data, aes(x = year_month, y = n, color = political_focus)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  labs(title = "Monthly Article Trends by Political Focus",
       x = "Publication Date", 
       y = "Number of Articles",
       color = "Political Focus") +
  theme_minimal() +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom")

print(p2)

# 3. Word count distribution (using sample for performance)
p3 <- ggplot(plot_data, aes(x = trump_related, y = word_count, fill = fox_related)) +
  geom_boxplot(alpha = 0.7, outlier.size = 0.5) +
  coord_cartesian(ylim = c(0, quantile(plot_data$word_count, 0.95, na.rm = TRUE))) +
  labs(title = "Word Count Distribution by Classification",
       subtitle = if(nrow(plot_data) < nrow(final_data)) paste("Based on random sample of", nrow(plot_data), "articles") else "",
       x = "Trump Classification",
       y = "Word Count", 
       fill = "Fox Classification") +
  theme_minimal() +
  scale_y_continuous(labels = comma)

print(p3)
```

### **Performance-Aware Validation**


```{r validation}
# Efficient validation for large datasets
cat("Performing data validation...\n")

# Check data quality
quality_check <- final_data %>%
  summarise(
    missing_titles = sum(is.na(title)),
    missing_dates = sum(is.na(publication_date)),
    zero_word_count = sum(word_count == 0, na.rm = TRUE),
    duplicate_titles = n() - n_distinct(title),
    .groups = 'drop'
  )

if(sum(quality_check) == 0) {
  cat("✓ Data quality check passed\n")
} else {
  kable(quality_check, caption = "Data Quality Issues Found")
}

# Performance metrics
processing_time <- Sys.time()
memory_usage <- format(object.size(final_data), units = "MB")

cat("Dataset processing completed\n")
cat("Final dataset size:", memory_usage, "\n")
cat("Processing timestamp:", format(processing_time), "\n")

# Export with compression for large files
write_csv(final_data, "large_political_news_analysis.csv")
cat("✓ Dataset exported successfully\n")
```

### **Conclusions for Large Dataset Analysis**

```r
## Findings and Recommendations

### Scalability Analysis Results

This large-scale analysis successfully processed `r nrow(final_data)` articles spanning `r max(final_data$publication_year) - min(final_data$publication_year)` years, demonstrating the effectiveness of automated political content classification at scale. The analysis identified `r sum(final_data$trump_related == "Trump-Related")` Trump-related articles and `r sum(final_data$fox_related == "Fox-Related")` Fox News-related articles, providing comprehensive coverage classification.

### Performance Optimizations Implemented

The analysis utilized several performance optimizations essential for large dataset processing: lazy evaluation with dtplyr, vectorized string operations, efficient memory management, and strategic data sampling for visualizations. These approaches enabled processing of substantial news corpora while maintaining analytical depth.

### Recommendations for Enterprise-Scale Implementation

**Infrastructure Scaling**: For datasets exceeding 100,000 articles, consider implementing distributed computing solutions using Apache Spark with sparklyr, or cloud-based processing platforms like AWS EMR or Google Cloud Dataproc.

**Real-Time Processing Pipeline**: Implement streaming analytics using Apache Kafka and real-time classification models to process incoming news articles continuously, enabling live political sentiment monitoring and bias detection.

**Advanced Machine Learning Integration**: Deploy transformer-based models (BERT, RoBERTa) for more nuanced classification, potentially improving accuracy from rule-based detection to contextual understanding of political content.

**Database Integration**: Migrate from CSV processing to scalable database solutions (PostgreSQL, MongoDB) with indexed text search capabilities for faster query performance on historical data analysis.

**Monitoring and Alerting**: Implement automated quality monitoring to detect classification drift, data anomalies, and processing performance degradation in production environments.

This scalable framework provides a foundation for enterprise-level news analysis applications, supporting real-time political monitoring, media bias research, and large-scale content classification systems.
